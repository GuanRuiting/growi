{"version":3,"file":"markdown-token-splitter.js","sources":["../../src/services/markdown-token-splitter.ts"],"sourcesContent":["import { encodingForModel, type TiktokenModel } from 'js-tiktoken';\n\nimport { splitMarkdownIntoFragments, type MarkdownFragment } from './markdown-splitter';\n\ntype MarkdownFragmentGroups = MarkdownFragment[][] ;\n\nfunction groupMarkdownFragments(\n    markdownFragments: MarkdownFragment[],\n    maxToken: number,\n): MarkdownFragmentGroups {\n\n  const prefixes = markdownFragments.map(({ label }) => {\n    if (label === 'frontmatter') return 'frontmatter';\n    const match = label.match(/^\\d+(?:-\\d+)*/)!; // eslint-disable-line @typescript-eslint/no-non-null-assertion\n    return match[0];\n  });\n\n  const uniquePrefixes = [...new Set(prefixes.filter(Boolean))];\n\n  // Group chunks by prefix\n  const fragmentGroupes: MarkdownFragmentGroups = [];\n  let remainingPrefixes = [...uniquePrefixes];\n\n  // Process chunks so that the total token count per level doesn't exceed maxToken\n  while (remainingPrefixes.length > 0) {\n    const prefix = remainingPrefixes[0]; // Get the first prefix\n    const hasNextLevelPrefix = uniquePrefixes.some(p => p !== prefix && p.startsWith(prefix));\n\n    if (!hasNextLevelPrefix) {\n      // If there is no prefix that starts with the current prefix, group the chunks directly\n      let matchingFragments = markdownFragments.filter(fragment => fragment.label.startsWith(prefix));\n\n      // Add parent heading if it exists\n      const parts = prefix.split('-');\n      for (let i = 1; i < parts.length; i++) {\n        const parentPrefix = parts.slice(0, i).join('-');\n        const parentHeading = markdownFragments.find(fragment => fragment.label === `${parentPrefix}-heading`);\n        if (parentHeading) {\n          matchingFragments = [parentHeading, ...matchingFragments]; // Add the heading at the front\n        }\n      }\n\n      fragmentGroupes.push(matchingFragments);\n    }\n    else {\n      // Filter chunks that start with the current prefix\n      let matchingFragments = markdownFragments.filter(fragment => fragment.label.startsWith(prefix));\n\n      // Add parent heading if it exists\n      const parts = prefix.split('-');\n      for (let i = 1; i < parts.length; i++) {\n        const parentPrefix = parts.slice(0, i).join('-');\n        const parentHeading = markdownFragments.find(fragment => fragment.label === `${parentPrefix}-heading`);\n        if (parentHeading) {\n          matchingFragments = [parentHeading, ...matchingFragments];\n        }\n      }\n\n      // Calculate total token count including parent headings\n      const totalTokenCount = matchingFragments.reduce((sum, fragment) => sum + fragment.tokenCount, 0);\n\n      // If the total token count doesn't exceed maxToken, group the chunks\n      if (totalTokenCount <= maxToken) {\n        fragmentGroupes.push(matchingFragments);\n        remainingPrefixes = remainingPrefixes.filter(p => !p.startsWith(`${prefix}-`));\n      }\n      else {\n        // If it exceeds maxToken, strictly filter chunks by the exact numeric prefix\n        const strictMatchingFragments = markdownFragments.filter((fragment) => {\n          const match = fragment.label.match(/^\\d+(-\\d+)*(?=-)/);\n          return match && match[0] === prefix;\n        });\n\n        // Add parent heading if it exists\n        for (let i = 1; i < parts.length; i++) {\n          const parentPrefix = parts.slice(0, i).join('-');\n          const parentHeading = markdownFragments.find(fragment => fragment.label === `${parentPrefix}-heading`);\n          if (parentHeading) {\n            strictMatchingFragments.unshift(parentHeading); // Add the heading at the front\n          }\n        }\n\n        fragmentGroupes.push(strictMatchingFragments);\n      }\n    }\n    remainingPrefixes.shift();\n  }\n\n  return fragmentGroupes;\n}\n\n// Function to group markdown into chunks based on token count\nexport async function splitMarkdownIntoChunks(\n    markdownText: string,\n    model: TiktokenModel,\n    maxToken = 800,\n): Promise<string[]> {\n  const encoder = encodingForModel(model);\n\n  // If the total token count for the entire markdown text is less than or equal to maxToken,\n  // return the entire markdown as a single chunk.\n  if (encoder.encode(markdownText).length <= maxToken) {\n    return [markdownText];\n  }\n\n  // Split markdown text into chunks\n  const markdownFragments = await splitMarkdownIntoFragments(markdownText, model);\n  const chunks = [] as string[];\n\n  // Group the chunks based on token count\n  const fragmentGroupes = groupMarkdownFragments(markdownFragments, maxToken);\n\n  fragmentGroupes.forEach((fragmentGroupe) => {\n    // Calculate the total token count for each group\n    const totalTokenCount = fragmentGroupe.reduce((sum, fragment) => sum + fragment.tokenCount, 0);\n\n    // If the total token count doesn't exceed maxToken, combine the chunks into one\n    if (totalTokenCount <= maxToken) {\n      const chunk = fragmentGroupe.map((fragment, index) => {\n        const nextFragment = fragmentGroupe[index + 1];\n        if (nextFragment) {\n          // If both the current and next chunks are headings, add a single newline\n          if (fragment.type === 'heading' && nextFragment.type === 'heading') {\n            return `${fragment.text}\\n`;\n          }\n          // Add two newlines for other cases\n          return `${fragment.text}\\n\\n`;\n        }\n        return fragment.text; // No newlines for the last chunk\n      }).join('');\n\n      chunks.push(chunk);\n    }\n    else {\n      // If the total token count exceeds maxToken, split content\n      const headingFragments = fragmentGroupe.filter(fragment => fragment.type === 'heading'); // Find all headings\n      const headingText = headingFragments.map(heading => heading.text).join('\\n'); // Combine headings with one newline\n\n      for (const fragment of fragmentGroupe) {\n        if (fragment.label.includes('content')) {\n          // Combine heading and paragraph content\n          const combinedTokenCount = headingFragments.reduce((sum, heading) => sum + heading.tokenCount, 0) + fragment.tokenCount;\n          // Check if headingChunks alone exceed maxToken\n          const headingTokenCount = headingFragments.reduce((sum, heading) => sum + heading.tokenCount, 0);\n\n          if (headingTokenCount > maxToken / 2) {\n            throw new Error(\n              `Heading token count is too large. Heading token count: ${headingTokenCount}, allowed maximum: ${Math.ceil(maxToken / 2)}`,\n            );\n          }\n\n          // If the combined token count exceeds maxToken, split the content by character count\n          if (combinedTokenCount > maxToken) {\n            const headingTokenCount = headingFragments.reduce((sum, heading) => sum + heading.tokenCount, 0);\n            const remainingTokenCount = maxToken - headingTokenCount;\n\n            // Calculate the total character count and token count\n            const fragmentCharCount = fragment.text.length;\n            const fragmenTokenCount = fragment.tokenCount;\n\n            // Calculate the character count for splitting\n            const charCountForSplit = Math.floor((remainingTokenCount / fragmenTokenCount) * fragmentCharCount);\n\n            // Split content based on character count\n            const splitContents = [];\n            for (let i = 0; i < fragment.text.length; i += charCountForSplit) {\n              splitContents.push(fragment.text.slice(i, i + charCountForSplit));\n            }\n\n            // Add each split content to the new group of chunks\n            splitContents.forEach((splitText) => {\n              const chunk = headingText\n                ? `${headingText}\\n\\n${splitText}`\n                : `${splitText}`;\n              chunks.push(chunk);\n            });\n          }\n          else {\n            const chunk = `${headingText}\\n\\n${fragment.text}`;\n            chunks.push(chunk);\n          }\n        }\n      }\n    }\n  });\n\n  return chunks;\n}\n"],"names":["groupMarkdownFragments","markdownFragments","maxToken","prefixes","label","uniquePrefixes","fragmentGroupes","remainingPrefixes","prefix","p","matchingFragments","fragment","parts","i","parentPrefix","parentHeading","sum","strictMatchingFragments","match","splitMarkdownIntoChunks","markdownText","model","encodingForModel","splitMarkdownIntoFragments","chunks","fragmentGroupe","chunk","index","nextFragment","headingFragments","headingText","heading","combinedTokenCount","headingTokenCount","remainingTokenCount","fragmentCharCount","fragmenTokenCount","charCountForSplit","splitContents","splitText"],"mappings":";;AAMA,SAASA,EACLC,GACAC,GACsB;AAExB,QAAMC,IAAWF,EAAkB,IAAI,CAAC,EAAE,OAAAG,QACpCA,MAAU,gBAAsB,gBACtBA,EAAM,MAAM,eAAe,EAC5B,CAAC,CACf,GAEKC,IAAiB,CAAC,GAAG,IAAI,IAAIF,EAAS,OAAO,OAAO,CAAC,CAAC,GAGtDG,IAA0C,CAAA;AAC5C,MAAAC,IAAoB,CAAC,GAAGF,CAAc;AAGnC,SAAAE,EAAkB,SAAS,KAAG;AAC7B,UAAAC,IAASD,EAAkB,CAAC;AAGlC,QAF2BF,EAAe,KAAK,CAAAI,MAAKA,MAAMD,KAAUC,EAAE,WAAWD,CAAM,CAAC,GAkBnF;AAEC,UAAAE,IAAoBT,EAAkB,OAAO,CAAAU,MAAYA,EAAS,MAAM,WAAWH,CAAM,CAAC;AAGxF,YAAAI,IAAQJ,EAAO,MAAM,GAAG;AAC9B,eAASK,IAAI,GAAGA,IAAID,EAAM,QAAQC,KAAK;AACrC,cAAMC,IAAeF,EAAM,MAAM,GAAGC,CAAC,EAAE,KAAK,GAAG,GACzCE,IAAgBd,EAAkB,KAAK,CAAAU,MAAYA,EAAS,UAAU,GAAGG,CAAY,UAAU;AACrG,QAAIC,MACkBL,IAAA,CAACK,GAAe,GAAGL,CAAiB;AAAA,MAE5D;AAMA,UAHwBA,EAAkB,OAAO,CAACM,GAAKL,MAAaK,IAAML,EAAS,YAAY,CAAC,KAGzET;AACrB,QAAAI,EAAgB,KAAKI,CAAiB,GAClBH,IAAAA,EAAkB,OAAO,CAAKE,MAAA,CAACA,EAAE,WAAW,GAAGD,CAAM,GAAG,CAAC;AAAA,WAE1E;AAEH,cAAMS,IAA0BhB,EAAkB,OAAO,CAACU,MAAa;AACrE,gBAAMO,IAAQP,EAAS,MAAM,MAAM,kBAAkB;AAC9C,iBAAAO,KAASA,EAAM,CAAC,MAAMV;AAAA,QAAA,CAC9B;AAGD,iBAASK,IAAI,GAAGA,IAAID,EAAM,QAAQC,KAAK;AACrC,gBAAMC,IAAeF,EAAM,MAAM,GAAGC,CAAC,EAAE,KAAK,GAAG,GACzCE,IAAgBd,EAAkB,KAAK,CAAAU,MAAYA,EAAS,UAAU,GAAGG,CAAY,UAAU;AACrG,UAAIC,KACFE,EAAwB,QAAQF,CAAa;AAAA,QAEjD;AAEA,QAAAT,EAAgB,KAAKW,CAAuB;AAAA,MAC9C;AAAA,IACF,OAxDyB;AAEnB,UAAAP,IAAoBT,EAAkB,OAAO,CAAAU,MAAYA,EAAS,MAAM,WAAWH,CAAM,CAAC;AAGxF,YAAAI,IAAQJ,EAAO,MAAM,GAAG;AAC9B,eAASK,IAAI,GAAGA,IAAID,EAAM,QAAQC,KAAK;AACrC,cAAMC,IAAeF,EAAM,MAAM,GAAGC,CAAC,EAAE,KAAK,GAAG,GACzCE,IAAgBd,EAAkB,KAAK,CAAAU,MAAYA,EAAS,UAAU,GAAGG,CAAY,UAAU;AACrG,QAAIC,MACkBL,IAAA,CAACK,GAAe,GAAGL,CAAiB;AAAA,MAE5D;AAEA,MAAAJ,EAAgB,KAAKI,CAAiB;AAAA,IAAA;AA2CxC,IAAAH,EAAkB,MAAM;AAAA,EAC1B;AAEO,SAAAD;AACT;AAGA,eAAsBa,EAClBC,GACAC,GACAnB,IAAW,KACM;AAKnB,MAJgBoB,EAAiBD,CAAK,EAI1B,OAAOD,CAAY,EAAE,UAAUlB;AACzC,WAAO,CAACkB,CAAY;AAItB,QAAMnB,IAAoB,MAAMsB,EAA2BH,GAAcC,CAAK,GACxEG,IAAS,CAAA;AAKC,SAFQxB,EAAuBC,GAAmBC,CAAQ,EAE1D,QAAQ,CAACuB,MAAmB;AAK1C,QAHwBA,EAAe,OAAO,CAACT,GAAKL,MAAaK,IAAML,EAAS,YAAY,CAAC,KAGtET,GAAU;AAC/B,YAAMwB,IAAQD,EAAe,IAAI,CAACd,GAAUgB,MAAU;AAC9C,cAAAC,IAAeH,EAAeE,IAAQ,CAAC;AAC7C,eAAIC,IAEEjB,EAAS,SAAS,aAAaiB,EAAa,SAAS,YAChD,GAAGjB,EAAS,IAAI;AAAA,IAGlB,GAAGA,EAAS,IAAI;AAAA;AAAA,IAElBA,EAAS;AAAA,MAAA,CACjB,EAAE,KAAK,EAAE;AAEV,MAAAa,EAAO,KAAKE,CAAK;AAAA,IAAA,OAEd;AAEH,YAAMG,IAAmBJ,EAAe,OAAO,CAAYd,MAAAA,EAAS,SAAS,SAAS,GAChFmB,IAAcD,EAAiB,IAAI,CAAAE,MAAWA,EAAQ,IAAI,EAAE,KAAK;AAAA,CAAI;AAE3E,iBAAWpB,KAAYc;AACrB,YAAId,EAAS,MAAM,SAAS,SAAS,GAAG;AAEhC,gBAAAqB,IAAqBH,EAAiB,OAAO,CAACb,GAAKe,MAAYf,IAAMe,EAAQ,YAAY,CAAC,IAAIpB,EAAS,YAEvGsB,IAAoBJ,EAAiB,OAAO,CAACb,GAAKe,MAAYf,IAAMe,EAAQ,YAAY,CAAC;AAE3F,cAAAE,IAAoB/B,IAAW;AACjC,kBAAM,IAAI;AAAA,cACR,0DAA0D+B,CAAiB,sBAAsB,KAAK,KAAK/B,IAAW,CAAC,CAAC;AAAA,YAAA;AAK5H,cAAI8B,IAAqB9B,GAAU;AAC3B+B,kBAAAA,IAAoBJ,EAAiB,OAAO,CAACb,GAAKe,MAAYf,IAAMe,EAAQ,YAAY,CAAC,GACzFG,IAAsBhC,IAAW+B,GAGjCE,IAAoBxB,EAAS,KAAK,QAClCyB,IAAoBzB,EAAS,YAG7B0B,IAAoB,KAAK,MAAOH,IAAsBE,IAAqBD,CAAiB,GAG5FG,IAAgB,CAAA;AACtB,qBAASzB,IAAI,GAAGA,IAAIF,EAAS,KAAK,QAAQE,KAAKwB;AAC7C,cAAAC,EAAc,KAAK3B,EAAS,KAAK,MAAME,GAAGA,IAAIwB,CAAiB,CAAC;AAIpD,YAAAC,EAAA,QAAQ,CAACC,MAAc;AAC7B,oBAAAb,IAAQI,IACV,GAAGA,CAAW;AAAA;AAAA,EAAOS,CAAS,KAC9B,GAAGA,CAAS;AAChB,cAAAf,EAAO,KAAKE,CAAK;AAAA,YAAA,CAClB;AAAA,UAAA,OAEE;AACG,kBAAAA,IAAQ,GAAGI,CAAW;AAAA;AAAA,EAAOnB,EAAS,IAAI;AAChD,YAAAa,EAAO,KAAKE,CAAK;AAAA,UACnB;AAAA,QACF;AAAA,IAEJ;AAAA,EAAA,CACD,GAEMF;AACT;"}